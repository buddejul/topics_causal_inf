\documentclass[11pt, a4paper, leqno]{article}
\usepackage{a4wide}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{float, afterpage, rotating, graphicx}
\usepackage{epstopdf}
\usepackage{longtable, booktabs, tabularx}
\usepackage{fancyvrb, moreverb, relsize}
\usepackage{eurosym, calc}
% \usepackage{chngcntr}
\usepackage{amsmath, amssymb, amsfonts, amsthm, bm}
\usepackage{caption}
\usepackage{mdwlist}
\usepackage{xfrac}
\usepackage{setspace}
\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\usepackage{minibox}
% \usepackage{pdf14} % Enable for Manuscriptcentral -- can't handle pdf 1.5
% \usepackage{endfloat} % Enable to move tables / figures to the end. Useful for some
% submissions.
\DeclareMathOperator{\E}{E}
\usepackage{multirow}

\usepackage[
    natbib=true,
    bibencoding=inputenc,
    bibstyle=authoryear-ibid,
    citestyle=authoryear-comp,
    maxcitenames=3,
    maxbibnames=10,
    useprefix=false,
    sortcites=true,
    backend=biber
]{biblatex}
\AtBeginDocument{\toggletrue{blx@useprefix}}
\AtBeginBibliography{\togglefalse{blx@useprefix}}
\setlength{\bibitemsep}{1.5ex}
\addbibresource{refs.bib}

\usepackage[unicode=true]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    anchorcolor=black,
    citecolor=NavyBlue,
    filecolor=black,
    menucolor=black,
    runcolor=black,
    urlcolor=NavyBlue
}


\widowpenalty=10000
\clubpenalty=10000

\setlength{\parskip}{1ex}
\setlength{\parindent}{0ex}
\setstretch{1.5}

\title{Topics Causal Inference}

\author{Julian Budde\thanks{Bonn University. Email: s93jbudd[at]uni-bonn.de}}

\date{
    \today
}

\begin{document}

\maketitle

\begin{abstract}
    This term paper presents simulation results for two connected approaches to estimating conditional average treatment effects (CATE):
    The \textit{causal forest} approach originally proposed in~\cite{wager2018estimation} and the \textit{generic machine learning} approach discussed in~\cite{chernozhukov2023genml}.
    In a first part, I replicate simulation results for causal forest reported in the original paper that demonstrate the ability to estimate a sparse CATE function and perform valid inference.
    I then continue to extend their analysis by lowering the degree of sparseness of the DGP and show that their method breaks down: RMSE is significantly increased and coverage is far below the nominal level.
    I show that inference on the BLP is still possible using the generic machine learning approach. However, for the DGP I consider the RMSE is still considerably worse, illustrating a trade-off between estimation properties and informativeness of the inferential target.
    As an additional point of interest I consider the design of high-dimensional monte carlo studies using the approach suggested by~\cite{athey2024wgan}.
\end{abstract}

\clearpage

\section{Introduction}

\section{Methods}

\subsection{Problem Setup}
Throughout we consider a setting with a binary treatment denoted by $D$ taking on values in $\{0, 1\}$.
$Z$ will denote a vector of covariates that has dimension $p$. The sample size will be noted by $n$.
We will consider generally consider a setting of \textit{unconfoundedness}, meaning that conditional on covariates the treatment is as-good-as randomly assigned.
Denoting the outcomes obtained in states of the world with treatment realization $D=d$ by $Y(d)$, this means
\begin{equation*}
    Y(0), Y(1) \perp D | Z.
\end{equation*}
In the context of~\cite{chernozhukov2023genml} we will additionally make the assumption that the \textit{propensity score} $p(z) \equiv \Pr(D=1|Z=z)$ is a known function.
The leading case of this is a stratified experiment, with $p(z) = 0.5$ being the special case of a completely randomized experiment.
Generally, the literature also often allows for an unknown propensity score which poses the problem of --- high-dimensional --- first-step estimation.
Note this is not the ``dimensionality'' issue considered here, which is about the complexity of conditional average treatment effects.
The target of this literature is the \textit{conditional average treatment effect} (CATE), which is the average treatment effect for individuals with covariates $Z=z$:
\begin{equation*}
    CATE(Z) = s_0(Z) = \E[Y(1) - Y(0)|Z].
\end{equation*}
Our goal is to estimate and perform inference on this quantity.

Ideally, we would like to have a procedure with the following \textit{properties}:
\begin{itemize}
    \item[(1)] Consistent estimation in the sense that the mean-squared error (MSE) converges to 0;
    \item[(2)] valid inference, i.e.\ confidence intervals, for $CATE(z)$;
    \item[(3)] ability to adapt to high-dimensional covariate spaces (regimes where $p$ grows with $n$) without making sparsity assumptions;
    \item[(4)] no functional form restrictions on $s_0(z)$.
\end{itemize}
However, we know from general results on optimal achievable non-parametric rates that such a procedure cannot exist.
All proposed approaches then proceed by relaxing one or more of the requirements stated above. The following briefly summarizes some of the ideas, a more comprehensive list of reference can be found in~\cite{causalml2024}.
I focus in particular on sparsity assumptions and approximations to the CATE function, because these are two ideas that are taken up in the generic machine learning approach discussed later.
\paragraph*{Sparsity}
A first strategy is to place sparsity assumptions on the CATE function.
For example, a common set of approaches postulates a linear conditional mean function:
\begin{equation*}
    \E[Y|Z] = \sum_{l=1}^{p_1}\alpha_l DX_l + \sum_{j=1}^{p_2}\beta_j W_j,
\end{equation*}
where the covariates $Z$ are split into those determining systematic heterogeneity of treatment effects, $X$, and controls, $W$.
In this model we have hence have $s_0(Z) = S_0(X) = \alpha'X$ under randomization. In these setups we can allow for both $p_1$ and $p_2$ to grow with the sample size.
Hence, the CATE function can be high-dimensional at least for a subset of the covariates.
For these linear models Lasso-type approaches adapted to causal inference (so called ``double'' lasso) will perform well, under a condition called \textit{approximate sparsity}.
Intuitively, this condition is fulfilled by DGPs where the magnitude of the (ordered) coefficients $\alpha_l, \beta_l$, decays sufficiently fast.
One such DGP is for example given by
\begin{equation*}
    \alpha_l = \frac{1}{l^2}, \qquad \beta_j = \frac{1}{j^2}.
\end{equation*}
Assuming $p_1$, the dimension of heterogeneity, does not grow too fast and we have approximate sparsity, it is possible to show an asymptotic normal approximation, namely
\begin{equation*}
    \sqrt{n}(\hat{\alpha}-\alpha) \sim_a N(0,V)
\end{equation*}
where $V$ can be consistently estimated. These results are shown in~\cite{belloni2015uniform} and~\cite{belloni2018uniformly}.

\paragraph*{Approximations to the CATE}
~\cite{semenova2021debiased} propose to focus on a \textit{low-dimensional} subset $X$ of the covariates $Z$, which the researcher has to decide on ex ante.
Then, the new target for inference becomes the \textit{partial} CATE $s_0(X)$.
Their paper then simultaneously allows for \textit{unconfoundedness} which poses the challenge of estimating $p(Z)$ --- potentially using machine learning methods --- in a first step.
In a second step, they the nfocus on a \textit{linear} approximation to the (partial) CATE\@.
Hence, they essentially use the following approximations to make the problem solvable:
\begin{equation*}
    s_0(Z) \approx s_0(X) \approx \beta'X.
\end{equation*}
The best linear predictor (BLP) problem in this case is given by
\begin{equation*}
    \min_{b\in\mathbb{R}^p} \E [(s_o(X) - X'b)^2].
\end{equation*}
We can then make use of the following identification result (which is sometimes referred to as the Horvitz-Thomson transform):
\begin{equation*}
    s_0(Z) = \E[YH|Z]
\end{equation*}
where $H = \frac{D-p(Z)}{p(Z)(1-p(Z))}$.
It is then easy to derive the solution of the BLP problem above as
\begin{equation*}
    \beta = E[XX']^{-1}E[XHY]
\end{equation*}
which instead of the BLP of $Y$ uses the transformed outcome $HY$.
For this we can then make use of standard least squares theory --- given that the nuisance parameter estimation in the first step satisfies certain regularity conditions --- and show
\begin{equation*}
    \hat{\beta} - \beta_0 \sim_a N(0, \Omega/N).
\end{equation*}

\subsection{Generic Machine Learning}
The generic ML approach allows want to place as little assumptions as possible on the data generating process, while maintaining valid inference.
This is achieved by (a) moving the inferential target to \textit{features} of the CATE (like the BLP) and (b) using sample splitting to derive inferential guarantees.
Hence, this comes at the cost of potentially being able to only make statements about a (bad) approximation to the original inferential target as well as a loss in precision due to sample splitting.
However, there are a number of clear advantages: The researcher does not have to commit to a specific --- small --- set of covariates ex-ante or make sparsity assumptions.
At the same time, the approach as discussed in~\cite{chernozhukov2023genml} does not allow for nuisance parameter estimation, i.e.\ requires a known propensity score.

\paragraph*{Two Stages}
A key feature of the approach is \textit{sample splitting}. Let $(M,A)$ denote two partitions of the sample into an auxiliary and a main dataset.
In Stage 1, we then use the auxiliary dataset $A$ to generate machine predictors for the maps $z\to B(z)$ (the BCA) and $z\to S(z)$ (the CATE).
In Stage 2, we then proceed to use $S(z), B(z)$ applied to the main dataset $M$ to make inference on \textit{features} of $z\to s_0(z)$, for example a best linear predictor.

\paragraph*{BLP of CATE}
The first feature is the best linear predictor of the CATE function \textit{by the machine learning proxy} $S(Z)$.
In particular, we want to solve
\begin{equation*}
    \min_{(b1,b2)\in \mathbb{R}^2} \E [(s_0(Z) - b_1 - b_2 S(Z))^2]
\end{equation*}
which, if it exists, is given by
\begin{equation*}
    BLP[s_0(Z)|S(Z)] := \beta_1 + \beta_2(S(Z) - \E S(Z)).
\end{equation*}
Here, the solutions are given by $\beta_1 = \E s_0(Z)$ and $\beta_2 = Cov[s_0(Z), S(Z)]/Var[S(Z)]$.

A simple approach to estimating $\beta_1, \beta_2$ is again the Horvitz-Thompson transform discussed above.
In particular, it is easy to show that the projection
\begin{equation*}
    HY = \mu_1 + \mu_2(S(Z) - \E S(Z)) + \varepsilon, \qquad \E[(1, S(Z) - \E S(Z))'\varphi] = 0
\end{equation*}
identifies the BLP parameters, i.e.\ $\mu_1=\beta_1$ and $\mu_2 = \beta_2$.
Estmation can then proceed based on the empirical analogue.
While inference \textit{conditional} on the split is standard, unconditional inference has to rely on median aggregation of many splits (discussed below).

\paragraph*{Other Targets}
Other potential targets beside the BLP of the CATE are sorted group average treatment effects (GATES) and classification analysis (CLAN).
GATES are average effects for a finite number of groups that are induced by the first stage machine learning proxy.
For example, we define five groups by the size of their predicted treatment effect $S(z)$.
More generally, we can consider any finite partition of the real line $-\infty = I_0 < I_1 < \cdots < I_K = \infty$ and the define GATES by
\begin{equation*}
    \gamma_k := \E[s_0(Z)|G_k], \qquad k=1,\ldots,K,
\end{equation*}
where group membership is determined by $G_k := I\{S(Z) \in I_k\}$.
In CLAN, we are interested in characteristics $g(Y, D, Z)$ and we want to compare those for the least ($G_1$) and most ($G_K$) affected groups:
\begin{equation*}
    \delta_g := \E[g(Y, D, Z)|G_g], \qquad g \in \{1, K\}.
\end{equation*}

\paragraph*{ML Method Selection}
[Tbd.]

\paragraph*{Split-based Inference}


\section{Simulation and Results}

\subsection{Software Packages}
For the baseline causal forest estimator I rely on the `CausalForest' implementation provided by the \href{https://econml.azurewebsites.net/}{EconML} Python package.
This package implements a range of machine learning techniques for causal based on the statistical literature and making use of existing machine learning implementations in Python.
For the generic ML part, I implemented most algorithms by hand since these only involve specifying linear regressions and sample splitting.
Again, it is possible to flexibly use any of the machine learning estimators in this case, for example implemented in EconML or sklearn directly.
For estimating DGPs using Wasserstein Generative Adversarial Networks (WGAN) I use the \href{https://github.com/gsbDBI/ds-wgan}{ds-wgan} implementation provided by the authors.
Unfortunately, the code is not maintained and not compatible with recent versions of most basic Python libraries (e.g. NumPy 2.0).
To avoid dependency conflicts with the other packages or manually debugging their code, I instead trained the generators in a Google Colab environment, which also comes at a significant speedup due to GPU availability.

\subsection{Simulation Setup}
\paragraph*{Designing a High-Dimensional DGP}
A key question is how to design a DGP that is both ``realistic'', in the sense that the results are relevant to empirical applications, and still poses a high-dimensional problem.
The typical approach taken in the literature, e.g.\ in~\cite{wager2018estimation} is to postulate a DGP that has convenient analytical features and is easy to simulate.
This simplifies the analysis but begs the question of relevance. Also, it leaves some degree of freedom to the researcher to choose a DGP that favourably displays the advantages of their proposed method.
To address this issue, ``empirical'' methods for simulation design have been proposed, such as the WGAN approach in~\cite{athey2024wgan}.
Without going into detail, the main idea is to train a \textit{generative} machine learning model, that can generate ``new'' observations by estimating the underlying DGP.\@
The training/optimization procedure can be thought of as a two-player game in which a \textit{generator} and a \textit{discriminator} compete in classifying observations as real or generated.

While this approach can be usefully employed to design specific simulations given a dataset at hand for some questions, it also has some limitations, specifically with respect to the problem studied here.
In particular, we are concerned with a high-dimensional, non-sparse structure of the CATE functional --- this is what the generic ML method is robust against.
However, for a given DGP with high-dimensional structure it cannot be possible train a generator that preserves this structure.
Else, if we had an estimate of the DGP, we could simply construct a plug-in estimator of the CATE, but as argued before such an estimator cannot exist.

For this reason I resort to the analytical DGP presented in~\cite{wager2018estimation} (WA in the following).
To illustrate the difficulty with using a WGAN generated DGP when the problem becomes high-dimensional, I also report results based on a generator that was trained on a draw from the WA dgp.
To the best of my knowledge, such simulation results are not available in the original or any other paper.
\footnote{They do, however, assess robustness of their simulation setup in several dimensions, including sampling variability, in Section 5.}
The following paragraphs explain the simulation setup in more detail.

\paragraph*{WA DGP}
I use the third DGP discussed in~\cite{wager2018estimation} as my baseline DGP\@.
Covariates are generated as $X~U([0, 1]^d)$ and potential outcomes are drawn from $Y^{(0,1)} \sim N(\E[Y^{(0,1)}|X], 1)$.
~\cite{wager2018estimation} refer to $d$ as the ``ambient dimension''. Note that in their simulations this is \textit{not} the dimensionality of the CATE function.
Instead, the CATE function is generated by
\begin{equation*}
    \tau(x) = \xi(X_1)\xi(X_2), \qquad \xi(x) = \frac{2}{1 + \exp^{-12(x-1/2)}}.
\end{equation*}
That is, the CATE function only depends on two covariates and the other $d-2$ covariates are pure noise (in terms of CATE estimation).
The propensity score is kept constant at 0.5.

\paragraph*{WGAN Approximation}
Describe how WGAN was performed.

\subsection{Results: WA Replication and WGAN}

\begin{table}
    \caption{Simulation Results: WA Replication and WGAN}
    \center
    \begin{tabular}{lccccc}
        & Dimension & MSE & SD & Coverage & SD \\
        \toprule
        \multicolumn{6}{l}{\textit{Panel A:\ Original WA DGP 3}} \\
        \input{../bld/wa_replication/tables/wa_replication_dgp3.tex} \\
        \midrule

        \multicolumn{6}{l}{\textit{Panel B:\ High Dimensional --- 4 Covariates}} \\
        \input{../bld/wa_replication/tables/wa_replication_dgp4.tex} \\
        \midrule

        \multicolumn{6}{l}{\textit{Panel C:\ High Dimensional --- 8 Covariates}} \\
        \input{../bld/wa_replication/tables/wa_replication_dgp5.tex} \\
        \midrule

        \multicolumn{6}{l}{\textit{Panel D:\ Placebo}} \\
        \input{../bld/wa_replication/tables/wa_replication_dgp6.tex} \\
        \bottomrule
    \end{tabular}

\end{table}

\subsection{Results: Generic Machine Learning}


\begin{table}
    \caption{Simulation Results: ML Generic --- Cate Estimation}
    \center
    \begin{tabular}{lcc}
        Dimension & MSE & SD \\
        \toprule
        \multicolumn{3}{l}{\textit{Panel A:\ Original WA DGP 3}} \\
        \input{../bld/generic_ml/tables/generic_ml_dgp3.tex} \\
        \midrule

        \multicolumn{3}{l}{\textit{Panel B:\ High Dimensional --- 4 Covariates}} \\
        \input{../bld/generic_ml/tables/generic_ml_dgp4.tex} \\
        \midrule

        \multicolumn{3}{l}{\textit{Panel C:\ High Dimensional --- 8 Covariates}} \\
        \input{../bld/generic_ml/tables/generic_ml_dgp5.tex} \\
        \midrule

        \multicolumn{3}{l}{\textit{Panel D:\ Placebo}} \\
        \input{../bld/generic_ml/tables/generic_ml_dgp6.tex} \\
        \bottomrule
    \end{tabular}

\end{table}

% Gates Figure
\begin{figure}
    \caption{Simulation Results: Generic ML --- GATES}\label{fig:gates}

    \centering
     \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../bld/generic_ml/plots/generic_ml_gates_dgp3.png}
         \caption{Original WA DGP 3}\label{fig_gates:dgp3}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../bld/generic_ml/plots/generic_ml_gates_dgp4.png}
         \caption{High-dimensional --- 4 Covariates}\label{fig_gates:dgp4}
     \end{subfigure}

     \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../bld/generic_ml/plots/generic_ml_gates_dgp5.png}
         \caption{High-dimensional --- 8 Covariates}\label{fig_gates:dgp5}
     \end{subfigure}
     \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../bld/generic_ml/plots/generic_ml_gates_dgp6.png}
         \caption{Placebo}\label{fig_gates:dgp6}
     \end{subfigure}

\end{figure}

% Gates Figure
\begin{figure}
    \caption{Simulation Results: Generic ML --- CLANS}\label{fig:clans}

    \centering
     \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../bld/generic_ml/plots/generic_ml_clans_dgp3.png}
         \caption{Original WA DGP 3}\label{fig_clans:dgp3}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../bld/generic_ml/plots/generic_ml_clans_dgp4.png}
         \caption{High-dimensional --- 4 Covariates}\label{fig_clans:dgp4}
     \end{subfigure}

     \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../bld/generic_ml/plots/generic_ml_clans_dgp5.png}
         \caption{High-dimensional --- 8 Covariates}\label{fig_clans:dgp5}
     \end{subfigure}
     \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{../bld/generic_ml/plots/generic_ml_clans_dgp6.png}
         \caption{Placebo}\label{fig_clans:dgp6}
     \end{subfigure}

\end{figure}

\clearpage
\newpage

% \setstretch{1}
\printbibliography
\setstretch{1.5}

% \appendix

% The chngctr package is needed for the following lines.
% \counterwithin{table}{section}
% \counterwithin{figure}{section}

\end{document}
